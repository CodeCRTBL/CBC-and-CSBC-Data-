\setlength{\parindent}{0pt}

Drug discovery is an interdisciplinary and multifaceted process which aims to find new therapeutic agents that could potentially aid or cure a specific disease without detrimental side effects \cite{araujo2018interdisciplinarity}. There are two main stream of study in this field, which are the traditional and modern methods. Traditional methods involve the “trial and error” approach, wherein the discovery of drugs are heavily dependent on natural sources, empirical data, analog design and serendipitous discoveries \cite{article}. An example of this are the study of natural products, which it involves the screening of compounds (i.e., extracted from plants, animals, and minerals) against a specific cell-line or bacteria \cite{ijms19061578}. Alternatively, local or internet based chemical libraries are used as a guide on finding drug leads. For instance, in the study of Sellamuthu et. al., the drug design process started with a known compound exhiniting inherent anti-tuberculosis properties. Base from this scaffold, an analog was synthesized and subsequently tested against the target cell-line \cite{sellamuthu2023analog}. Notably, the study identified novel lead compounds and uncovered a new therapeutic application. In general, the traditional methods are proven to work, however, data shows that the discovery through this path would likely take 10 – 15 years, which entails an average cost of over $1-2$ billion \cite{sun202290}. In addition, it is identified that the number of drugs produced through this method is decreasing, which possibly suggest that it is not sufficient to effectively discover new potential drug candidates \cite{pinzi2024drug}.  

On the other hand, modern drug discovery methods involve High-Throughput Screening (HTS), Fragment-Based Drug Discovery (FBDD), Structure-Based Drug Design (SBDD), Computational Drug Discovery (CDD), Phenotypic Drug Discovery (PDD), Artificial Intelligence (AI) and ML based techniques \cite{article}. 
Among these methods, HTS, FBDD, SBDD, and PBDD are known to be experimentally driven while CDD, ML and AI are known to be inherently data driven, computationally intensive and grounded in theoretical principles. \autoref{tab:modern_methods} summarizes their advantages, and presents challenges that until today remains unresolved.  

\begin{table}[h] 
	\centering
	\begin{threeparttable}
		\renewcommand{\arraystretch}{1.2} 
		\small
		\begin{tabular}{p{4cm} p{6cm} p{6cm}}
			\hline
			\textbf{Method} & \textbf{Advantages} & \textbf{Disadvantages} \\
			\hline
			HTS \cite{martis2011highHTS} & Rapid evaluation of compounds for biological activity Can test up to hundreds of thousands/day.& expensive cost, requires extensive libraries, suffers from high false-positive rates. \\
			
			FBDD \cite{chen2025fragment} & Narrows down the analysis on small molecular fragments, which results to optimization potent drugs. & Requires complex strategist and techniques on fragment-linking and advanced structural biology. \\
			
			SBDD \cite{batool2019structure} & Optimizes binding affinity via fitting of drugs to a target. & Availability of high-resolution structural data. \\
			
			CDD \cite{batool2019structure} & Significant reduction of experimental work, cost and time by using simulations and predictive models to screen potential compounds. & Model predictive capacity heavily relies on quality of input data and computational power. \\
			
			PDD \cite{garaci2024PDD} & focuses on observing changes in disease phenotypes rather than molecular targets. & Mechanistic road map is unclear, requires additional validations. \\
			
			ML \cite{ammad2014integrative} & Flexible, can be combined with different techniques (e.g., QSAR), efficient on predicting drug-targets and molecular properties. & good quality data input, sensitive to imbalanced data set, complexity in integrating to different systems. \\
			
			AI \cite{article} & Automation of drug discovery, and other systems alike & Complexity of integration into different systems, Difficulty on interpreting and validating AI driven predictions. \\
			\hline
		\end{tabular}
	\end{threeparttable}
	\caption{Comparison of Modern Drug Discovery Methods} 
	\label{tab:modern_methods} % Fixed label
\end{table}

The effective integration of AI and ML particularly in virtual drug screening, heavily relies on QSAR. QSAR is a computational technique that uses statistical tools to explain the observed structure variation in relation to the target activity \cite{ammad2014integrative}. The crucial step on performing QSAR starts on selecting a good database (e.g., ChEMBL, ChemSpider, DrugBank, and etc) that contains vital information about the molecule of interest. This information might entail 1D (e.g., SMILES format), 2D, 3D structural representations, bioactivity and etc. These information can be extracted via downloading or scrapping. From the downloaded/scrapped information variables of interest are identified and significant ones are selected --- base on its effect to the target activity. The most common variables used in this field of study are molecular fingerprints (MFP). Molecules MFP can be generated through various methods: a) substructure keys-based fingerprints (i.e., MACSS, PubChem fingerprint, BCI fingerprints, etc.); b) Topological or path-based fingerprints; and c) Circular fingerprints \cite{cereto2015MF}. The selection of an appropriate method is a pivotal step in QSAR model development --- as it implicitly dictates the models predictive capacity, thus, substantial effort is devoted to this phase. 

Substructure keys-based fingerprints uses a pre-defined structural keys that are match to a target molecule in a form of bit. The presence of pre-defined structure on the target compound is translated as 1 and absence is 0, creating a bit strings of binary numbers \cite{christie1990MACCS}. On the other hand, topological fingerprints uses linear path on fragmenting a molecule to generate MFP. This means that MFP's generated from this method will be depended on a given data set. However, a bit from this method might carry more than one meaning if the resolution is not high enough --- this phenomena is called bit collision \cite{cereto2015MF}. Circular fingerprint can also be classified as topological fingerprints, however, instead of linear paths, the environment of each atom are recorded base on circular path radius. The advantage of this technique over the two are: a) MFP are not fixed, hence, identification of new meaningful fingerprints is possible; b) it can detect all of the structural features of a given molecule (i.e., linear and non-linear features), in which topological fingerprints fails --- this opens up for higher possibility of detecting new structural patterns \cite{rogers2010extended}. An example of algorithm that uses this technique is Morgan Fingerprinting \cite{morgan1965generation}. To make quantification possible from the structure queries and fingerprints, similarity analysis are performed via calculation of similarity coefficients. Often used for this task are Tanimoto/Jaccard coefficient, and Euclidean distance. Observations from these are combined with different mathematical models to create Machine Learning Algorithm (MLA) that can produce an accurate and reliable predictions against the target activity \cite{keith2021qSAR}. Given the intricacies of QSAR-ML building procedues, creation of it remains a big challenge \cite{gao2023uni-qsar}.
%Common challenges in traditional methods including: a) identifying viable drug leads; b) reduction of cost, time and experiments demands; and c) improving success rate --- are generally mitigate by modern methods. For instance, HTS, FBDD, SBDD and PDD can experimentally hasten identification of viable drug leads by means of using chemical libraries, structure and phenotypic information. Moreover, experimental demands are significantly reduced and the success rate of drug discovery increases with the aid of AI and ML predictions.

Despite all of the efforts made over the years on drug discovery the following key challenges remains: a) high experimental and computational demands; b) difficulty of building a QSAR-ML model that exhibits a good prediction capacity against target activity; c) QSAR - AI/ML integration complexity; and d) interpretability and validation. In line with this, the study aims to focus on the following: a) to reduced the complexity of QSAR-ML development by establishing a new method on selecting MFP; b) to create a QSAR-ML model that can classify molecules bioactivity against HCT-116 using only 2D-structural motifs; and c) to provide an interpretation and validation through structural analysis and random testing. To make this work the following assumptions are formulated: a) there might be MFP's that directly affects the target bioactivity (significant bits); b) the absence and presence of significant bits might dictates molecules bioactivity; c) significant bits might have position and neighbor dependency; and d) given the first three assumptions true, these significant bits can be used as features in QSAR-ML classification models which are expected to have good predicting capacity.    

For the computational work, the study proposes to start blindly --- this means that no molecular targets are set, and directly proceed on downloading/scrapping 2D structures of molecules with particular bioactivity. This allows to remove intrinsic biases and data complexity. From the downloaded 2D structures significant bits are generated via Crude Bits Counting (CBC) and Cluster-Subtraction Bits Counting (CSBC).CBC is a combination of Morgan Fingerprint Algorithm (MFA) and bit frequency counting, wherein it assumes that the most frequently observed bits are responsible for molecules bioactivity. Whereas, in CSBC aside from MFA and bit counting, an extra step of segregation, structure subtraction and bit frequency distribution analysis are performed. These additional extra steps are expected to inherently capture position and neighbor dependencies among the selected significant bits. The generated significant bits from the two methods are used as features for the development of QSAR-ML models: CBC-ML and CSBC-ML. 


%Compared to mentioned modern techniques that entails identifying first the target (i.e., protein, genes and etc), this study propose to not start on any target sites, but instead to start on data base scrapping/downloading of 2D structures with a particular bioactivity. By doing so, it removes prior biases and data complexity. Once the 2D structural and bioactivity information are extracted, molecular fingerprinting via Morgan Fingerprinting Algorithm (MFA) will be applied to generate unique MFP (bits). These bits are hypothesize to contain the significant parts of the compounds that is related to its bioactivity. If correct then, the following are assumed to be true: a) molecules activity is based on the presence and absence of these unique bits, hence, it might be of a high frequency; and b) in case of positional and neighbor dependency, these unique bits might be observe present throughout a given data set. 

%Two methods are develope to extract the molecular fingerprints which are the Crude Bits Counting (CBC) and Cluster-Subtraction Bits Counting (CSBC). CBC involves the logic of linear relationship between the significant bits and target bioactivity, wherein the presence and absence of these bits  are assume to dictates the bioactivity of target compound. On the other hand, CSBC involves segregation of compounds through clustering and structure subtraction. Both CSBC and CBC involves bit frequency counting and ranking  were the top bits were selected based on its frequency in the data set. However, CSBC has an extra step of comparsion to see whether the top candidates are commonly observed across a given data set. Top bits from CBC and CSBC, will be used to QSAR-ML studies, in which two models will be created --- CBC-ML and CSBC-ML. This machine learning models are expected to effectively classify the compounds activity or inactivity against HCT-116. If successfully created, it will be trained and test against molecules with or without bioacitvity against HCT-116. To measure models performance confusion matrix parameters, AUC scores and ROC curve will be used. The best model with a good classification capacity will be recommended for further robust testing.

%HTS is an experimental based approach in drug discovery, which starts on selecting first the target, followed by testing of the compounds coming from chemical libraries, identification hits and drug candidates and finally proceeding to clinical trials \cite{martis2011highHTS}. The main advantage of this method is it offers to test hundreds of thousand of compounds within the span of a day, however, the cost to build the equipment and resources for this method is huge and it suffers from producing false positive and negative errors. 

%Another method that commonly used in drug discovery is FBDD, however, unlike HTS it focuses on small fragments (MW $<$ 300 Da) which potentially have an interaction with the target binding site. Essentially, in FBDD the target binding site is established first followed by creation of small molecule fragments database \cite{chen2025fragment}. These small fragments were test against the target binding site to identify the Ligand Efficiency (LE) and validated through various biophysical, biochemical and computational approach. Once it is validated, the hit will be used to further lead to the optimization of structure and finally development of a drug for a clinical trial. Compared to HTS, FBDD requires fewer number of compounds to find hits and it can also potentially identify novel binding sites. However, the method is quite complex for it involves sophisticated techniques such as NMR and X-ray crystallography. Moreover, analysis of binding affinity of small molecule fragments to binding site is often hard for its signal is weak and almost comparable to noise. 

%SBDD starts on generation human genome sequence, followed by extraction of proteins, and then from there, the therapeutically important proteins are identified and their structure gets elucidated. After, the preparation of active compounds database takes place and the identification of druggable target protein and its binding sites. Consequently, the screening and docking of active compounds against the binding cavity of target protein are performed. Results from screening and docking hits will be rank based on their binding affinities, and the top compounds are selected to proceed to synthesis and clinical trials \cite{batool2019structure}.

%CDD is essentially combination of SBDD and Ligand-Based Drug Discovery (LBDD). Known protein structure and ligands are assessed and hit against the database of active compounds, through this method, virtual screening takes place and the hits are identified. After, molecular docking and dynamics are performed to verify the interaction and identify the LE of the hits. Lastly, quantitative structural activity relationship (QSAR) modelling is created, which is later used to generate prediction with regards to the bioactivity of the compound against the target site \cite{schaduangrat2020CDD}. The main advantage of this technique over the experimental-based (e.g., HTS, FBDD, SBDD, and PDD) is that it streamlines the procedure of drug discovery with lesser cost and reduced physical testing. 

%PDD is a method that identifies small molecules or peptides that possibly alter the desired phenotype of cell organisms. Generally, PDD method starts on selection of model system, which should be ideally closely or totally reflects the phenotype of target disease. After selection, development of assay takes place wherein it involves measuring cell viability, changes in protein expression and etc. When development of assay is successful, compounds from libraries will be screened against it to identify which changes the phenotype significantly. Once there is a hit, lead compound will be validated through testing the model system with its various concentration. Validated hits will undergo mechanism studies to understand and identify the interaction between the compound and the biological target(s). Despite the laborious task of assaying, PDD offers to unveil novel and complex mechanism of compounds against complex biological system of target cells. Conversely, since the PDD method does not require prior knowledge to exact biological targets, its identification poses a challenged, which lead to extensive testing and verification \cite{garaci2024PDD}. 

 





