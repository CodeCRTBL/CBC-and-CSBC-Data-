\setlength{\parindent}{0pt}

Drug discovery is an interdisciplinary and multifaceted field which aims to find new therapeutic agents that could potentially aid or cure a specific disease without detrimental side effects \cite{araujo2018interdisciplinarity}. There are two modes of approach in drug discovery. Traditional methods involve a “trial-and-error” approach, where the discovery of drugs are heavily dependent on natural sources, empirical data, analog design and serendipitous discoveries \cite{article}. An example of this is the study of natural products, which involves the screening of compounds (i.e., extracted from plants, animals, and minerals) against a specific cell-line or bacteria done by Thomford and his co-workers \cite{ijms19061578}. Alternatively, local or internet based chemical libraries are used as a guide on finding drug leads. As an example, the drug design process of Sellamuthu et. al. starts with a known compound exhibiting inherent anti-tuberculosis properties. Based on their design process, an analog was synthesized and subsequently tested against the target cell-line \cite{sellamuthu2023analog}. Notably, the study identified novel lead compounds and uncovered a new therapeutic application. Although traditional methods are proven to work, data shows that the discovery through this path would likely take 10 – 15 years and an average cost of over $1-2$ billion \cite{sun202290}. In addition, it was shown that the number of drugs produced through traditional methods is decreasing, suggesting that it is not sufficient to effectively discover new potential drug candidates \cite{pinzi2024drug} especially in a rapidly scaling economy.  

On the other hand, modern drug discovery methods involve High-Throughput Screening (HTS), Fragment-Based Drug Discovery (FBDD), Structure-Based Drug Design (SBDD), Computational Drug Discovery (CDD), Phenotypic Drug Discovery (PDD), Artificial Intelligence (AI) and ML based techniques \cite{article}. The methods HTS, FBDD, SBDD, and PBDD are known to be experimentally driven while CDD, ML and AI are known to be inherently data driven, computationally intensive and grounded on theoretical principles. \autoref{tab:modern_methods} summarizes their advantages, and their present challenges.  

\begin{table}[h] 
	\centering
	\begin{threeparttable}
		\renewcommand{\arraystretch}{1.2} 
		\small
		\begin{tabular}{p{4cm} p{6cm} p{6cm}}
			\hline
			\textbf{Method} & \textbf{Advantages} & \textbf{Disadvantages} \\
			\hline
			HTS \cite{martis2011highHTS} & Rapid evaluation of compounds for biological activity. Can test up to hundreds of thousands/day.& Expensive cost, requires extensive libraries, suffers from high false-positive rates. \\
			
			FBDD \cite{chen2025fragment} & Narrows down the analysis to small molecular fragments, which results to optimization of potential drugs. & Requires complex strategies and techniques on fragment-linking and advanced structural biology. \\
			
			SBDD \cite{batool2019structure} & Optimizes binding affinity via fitting of drugs to a target. & Availability of high-resolution structural data. \\
			
			CDD \cite{batool2019structure} & Significant reduction of experimental work, cost and time by using simulations and predictive models to screen potential compounds. & Model predictive capacity heavily relies on quality of input data and computational power. \\
			
			PDD \cite{garaci2024PDD} & Focuses on observing changes in disease phenotypes rather than molecular targets. & Mechanistic road map is unclear, requires additional validations. \\
			
			ML \cite{ammad2014integrative} & Flexible and can be combined with different techniques (e.g., QSAR), efficient in predicting drug-targets and molecular properties. & Requires good quality data input, sensitive to imbalanced data set, complexity in integrating to different systems. \\
			
			AI \cite{article} & Automation of drug discovery and other similar systems & Complexity of integration into different systems and difficulty in interpreting and validating predictions. \\
			\hline
		\end{tabular}
	\end{threeparttable}
	\caption{Comparison of modern drug discovery methods} 
	\label{tab:modern_methods} % Fixed label
\end{table}

The effective integration of AI and ML particularly in virtual drug screening, heavily relies on QSAR. QSAR is a computational technique that uses statistical tools to explain the observed structure variation in relation to the target activity \cite{ammad2014integrative}. The crucial step in QSAR is the selection of a good database (e.g., ChEMBL, ChemSpider, DrugBank, and etc) that contains vital information about the candidate molecules. Extracted variables of interest are identified and those posited to notably relate to the target activity are selected. The most common variables used in QSAR are molecular fingerprints. These can be generated through various methods: a) substructure keys-based fingerprints (i.e. MACSS, PubChem fingerprint, BCI fingerprints, etc.); b) topological or path-based fingerprints; and c) circular fingerprints \cite{cereto2015MF}. The selection of an appropriate molecular fingerprint is a pivotal step in QSAR model development as it implicitly dictates the models predictive capacity. Substantial effort is devoted to its optimization and development. 

Substructure keys-based fingerprints use pre-defined structural keys that are matched to a target molecule in a form of binary representations, or bits, \cite{christie1990MACCS}. On the other hand, topological fingerprints uses a linear path in fragmenting a molecule to generate fingerprints. Bits generated from this method might carry more than one meaning if the resolution is not high enough --- a phenomena referred to as bit collision \cite{cereto2015MF}. Circular fingerprints can also be classified as topological fingerprints, with the fundamental difference that instead of using linear paths, the environment of each atom are recorded based on a radial path. The advantage of this technique over the other two are: 1) fingerprints are not fixed, making identification of new meaningful fingerprints possible, and 2) it can detect all of the structural features of a given molecule (i.e., linear and non-linear features) where topological fingerprints fails. This opens up to a higher possibility of detecting new structural patterns \cite{rogers2010extended}. An example of a circular fingerprinting algorithm is Morgan Fingerprinting (MFP) \cite{morgan1965generation}. 

To make quantification possible from the structural queries, similarity analysis is performed via calculation of coefficients. Often used for this purpose are Tanimoto/Jaccard coefficients and Euclidean distances. Observations from these are combined with different mathematical models to create Machine Learning Algorithms (MLA) that can produce accurate and reliable predictions against the target activity \cite{keith2021qSAR}. Unfortunately, the intricacies of the QSAR-ML building procedures, and its creation remains a big challenge \cite{gao2023uni-qsar}.
%Common challenges in traditional methods including: a) identifying viable drug leads; b) reduction of cost, time and experiments demands; and c) improving success rate --- are generally mitigate by modern methods. For instance, HTS, FBDD, SBDD and PDD can experimentally hasten identification of viable drug leads by means of using chemical libraries, structure and phenotypic information. Moreover, experimental demands are significantly reduced and the success rate of drug discovery increases with the aid of AI and ML predictions.

Despite all of the efforts made over the years on drug discovery the following key challenges persist: 1) high experimental and computational demands; 2) difficulty of building a QSAR-ML model that exhibits good predictive capacity against target activity; 3) QSAR - AI/ML integration complexity; and 4) interpretability and validation of results. To address some of these challenges, the current study was aimed at the following objectives: 1) to reduce the complexity of QSAR-ML development by establishing a new method for generating molecular fingerprints; 2) to create a QSAR-ML model that can classify molecular bioactivity using only 2D-structural motifs; and 3) to provide interpretation and validation through structural analysis and random testing. These goals have the following concomitant assumptions: a) there might be molecular fingerprints that directly affect  bioactivity (significant bits); b) the absence and presence of significant bits may also dictate bioactivity; and c) significant bits may incorporate dependency in position and structural environment.    

%The computational work started with no molecular targets set, downloading/scrapping 2D structures of molecules with prescribed bioactivity. This allowed the removal of intrinsic biases and data complexity. From the downloaded 2D structures significant bits are generated via Crude Bits Counting (CBC) and Cluster-Subtraction Bits Counting (CSBC).CBC is a combination of Morgan Fingerprint Algorithm (MFA) and bit frequency counting, wherein it assumes that the most frequently observed bits are responsible for molecules bioactivity. Whereas, in CSBC aside from MFA and bit counting, an extra step of segregation, structure subtraction and bit frequency distribution analysis are performed. These additional extra steps are expected to inherently capture position and neighbor dependencies among the selected significant bits. The generated significant bits from the two methods are used as features for the development of QSAR-ML models: CBC-ML and CSBC-ML. 


%Compared to mentioned modern techniques that entails identifying first the target (i.e., protein, genes and etc), this study propose to not start on any target sites, but instead to start on data base scrapping/downloading of 2D structures with a particular bioactivity. By doing so, it removes prior biases and data complexity. Once the 2D structural and bioactivity information are extracted, molecular fingerprinting via Morgan Fingerprinting Algorithm (MFA) will be applied to generate unique MFP (bits). These bits are hypothesize to contain the significant parts of the compounds that is related to its bioactivity. If correct then, the following are assumed to be true: a) molecules activity is based on the presence and absence of these unique bits, hence, it might be of a high frequency; and b) in case of positional and neighbor dependency, these unique bits might be observe present throughout a given data set. 

%Two methods are develope to extract the molecular fingerprints which are the Crude Bits Counting (CBC) and Cluster-Subtraction Bits Counting (CSBC). CBC involves the logic of linear relationship between the significant bits and target bioactivity, wherein the presence and absence of these bits  are assume to dictates the bioactivity of target compound. On the other hand, CSBC involves segregation of compounds through clustering and structure subtraction. Both CSBC and CBC involves bit frequency counting and ranking  were the top bits were selected based on its frequency in the data set. However, CSBC has an extra step of comparsion to see whether the top candidates are commonly observed across a given data set. Top bits from CBC and CSBC, will be used to QSAR-ML studies, in which two models will be created --- CBC-ML and CSBC-ML. This machine learning models are expected to effectively classify the compounds activity or inactivity against HCT-116. If successfully created, it will be trained and test against molecules with or without bioacitvity against HCT-116. To measure models performance confusion matrix parameters, AUC scores and ROC curve will be used. The best model with a good classification capacity will be recommended for further robust testing.

%HTS is an experimental based approach in drug discovery, which starts on selecting first the target, followed by testing of the compounds coming from chemical libraries, identification hits and drug candidates and finally proceeding to clinical trials \cite{martis2011highHTS}. The main advantage of this method is it offers to test hundreds of thousand of compounds within the span of a day, however, the cost to build the equipment and resources for this method is huge and it suffers from producing false positive and negative errors. 

%Another method that commonly used in drug discovery is FBDD, however, unlike HTS it focuses on small fragments (MW $<$ 300 Da) which potentially have an interaction with the target binding site. Essentially, in FBDD the target binding site is established first followed by creation of small molecule fragments database \cite{chen2025fragment}. These small fragments were test against the target binding site to identify the Ligand Efficiency (LE) and validated through various biophysical, biochemical and computational approach. Once it is validated, the hit will be used to further lead to the optimization of structure and finally development of a drug for a clinical trial. Compared to HTS, FBDD requires fewer number of compounds to find hits and it can also potentially identify novel binding sites. However, the method is quite complex for it involves sophisticated techniques such as NMR and X-ray crystallography. Moreover, analysis of binding affinity of small molecule fragments to binding site is often hard for its signal is weak and almost comparable to noise. 

%SBDD starts on generation human genome sequence, followed by extraction of proteins, and then from there, the therapeutically important proteins are identified and their structure gets elucidated. After, the preparation of active compounds database takes place and the identification of druggable target protein and its binding sites. Consequently, the screening and docking of active compounds against the binding cavity of target protein are performed. Results from screening and docking hits will be rank based on their binding affinities, and the top compounds are selected to proceed to synthesis and clinical trials \cite{batool2019structure}.

%CDD is essentially combination of SBDD and Ligand-Based Drug Discovery (LBDD). Known protein structure and ligands are assessed and hit against the database of active compounds, through this method, virtual screening takes place and the hits are identified. After, molecular docking and dynamics are performed to verify the interaction and identify the LE of the hits. Lastly, quantitative structural activity relationship (QSAR) modelling is created, which is later used to generate prediction with regards to the bioactivity of the compound against the target site \cite{schaduangrat2020CDD}. The main advantage of this technique over the experimental-based (e.g., HTS, FBDD, SBDD, and PDD) is that it streamlines the procedure of drug discovery with lesser cost and reduced physical testing. 

%PDD is a method that identifies small molecules or peptides that possibly alter the desired phenotype of cell organisms. Generally, PDD method starts on selection of model system, which should be ideally closely or totally reflects the phenotype of target disease. After selection, development of assay takes place wherein it involves measuring cell viability, changes in protein expression and etc. When development of assay is successful, compounds from libraries will be screened against it to identify which changes the phenotype significantly. Once there is a hit, lead compound will be validated through testing the model system with its various concentration. Validated hits will undergo mechanism studies to understand and identify the interaction between the compound and the biological target(s). Despite the laborious task of assaying, PDD offers to unveil novel and complex mechanism of compounds against complex biological system of target cells. Conversely, since the PDD method does not require prior knowledge to exact biological targets, its identification poses a challenged, which lead to extensive testing and verification \cite{garaci2024PDD}. 

 





