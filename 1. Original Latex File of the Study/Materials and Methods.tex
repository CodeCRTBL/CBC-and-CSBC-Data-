\setlength{\parindent}{0pt}

\subsection*{Data Collection and Processing}
The structural and bioactivity information of compounds against HCT-116 that were used in the study were extracted to ChEMBL website. A total of   12910 molecules with \% Inhibition data were collected. Data with missing \% Inhibition and those that fall above the 95\% quantile of the were removed. This left only a total of 6304 molecule which is amenable to the study purposes. In case of imbalance sample representation, Synthetic Minority Over-sampling Technique (SMOTE) is used \cite{gomatam2024SMOTE}. This technique involves interpolation of synthetic samples for the minority class to balance the minority and majority classes \footnote{Minority Class: $<$ 0 \% Inhibition (0), Majority Class: $>$ 0 \%Inhibition (1)}.   
%To avoid statistical error and biased prediction, the data undergoes preprocessing, which entails the removal of incomplete data and outliers. The factors considered in preprocessing were the following: a) missing values of \% Inhibition; and b) the data falls above the 95\% quantile of the data set. After preprocessing, only 6304 molecules were left, and it was used as a clean data set of the study. 

%Distribution of the uncleaned data set (Figure X) were skewed to the right, suggesting that the data are not symmetrically distributed which could lead to misleading and inaccurate predictions. Therefore, the removal of outliers (\% Inhibition greater than the 95 \% quantile and missing values) are necessary, Figure X.2 shows the improvement of data distribution which is more normal compared to uncleaned data. However, this cleaned data consist of only 6304 molecules.  

%\includegraphics[scale = 0.5]{clustering_analysis_plot.png} 

\subsection*{Generation and Visualization of Morgan Profiles and Morgan Fingerprints}

The molecules were converted into Mols from their initial SMILES format to be able to generate Morgan Fingerprints (MF) and Morgan Profiles (MP) using MFA. The way MFA works is through circular fragmentation --- where central atom is randomly assign, and through this MF's are produce. Unique bit numbers from 0-2047 are randomly assign to each MF's, thus, giving it an identification (i.e., bit ID). To create MP's, a series of structural queries is perform until a shape of 1x2048 is achieved (\autoref{fig:mpex}).

\begin{figure}[htbp!] % 'h' places the figure approximately here
	\centering
	\includegraphics[scale = 0.25]{Mprofile2.png}% Replace with your image filename
	\caption{Fragmentation of Target Molecule to Produce MF and MP}
	\label{fig:mpex} % Optional: use \label for referencing
\end{figure} 

The generated MP 1-D matrix which consist of binary numbers, was used as the 2D-structure  representation of each molecule, while the MF was used as representation of molecular fingerprints/morgan fingerprints (i.e., fragments). To check the conversion success, MP's and MF's were reconverted back to their respective 2D-Structures and saved as png files.    
%after which, Morgan Fingerprinting Algorithm (MFA) parameters were set to: a) radius = 4; b) nBits = 2048; c) UseFeatures = True; and d) useChirality = True. After setting the algorithm, converted data in Mols format are feed to it which results to  generation of compounds  MP and MF. The Generated MP will become the 1-D matrix (1x2048) representation of the overall 2D-structure of each compound, which  consist of binary numbers, whereas, the MF will be the representation of molecular fingerprints/ morgan fingerprints (i.e., fragments) which were recognized as bits by the machine.

%To check if the conversion of 6304 compounds 2D-structure were successful, molecular drawing was performed through rdkit draw package. In this procedure, compounds MF's and MP's were reconverted back to its respective 2D-structures and saved as png file. In this way, the researchers were able to check the success of MP's and MF's generation via MFA. All of the data generated from these procedures were used to study the Quantitative-Structural Activity Relationship of the compounds against HCT-116.  

\subsection*{Structural Activity Relationship (SAR) of HCT-116 Compounds}
%\subsubsection{}
MF and MP 1-D matrices and vector representations were used to set the ML environment. Once set, the 2D-structural data inputs can now be readily perceived and interpreted by the machine. However, at this point there was still no way to determine which bits were significant --- there is no assessment on the weights of each bits relative to the bioactivity against HCT-116. Thus, the generated MF's were first analyzed and categorized before feeding it to the MLA. In this study, two methods were implemented to categorize and analyze the bits: CBC and CSBC.
  
%cannot identify which among the bits were significant and insignificant, hence, it lacks the ability to assess the weights of each bits relative to its bioacitivity against HCT-116. Therefore, the generated MF's were first analyzed and categorized before feeding it to the machine. In this study, two methods were implemented to categorized and analyzed the bits which are Crude Bits Counting (CBC), and Cluster-Subtraction Bits Counting (CSBC). 

\begin{figure*}[h] % 'h' places the figure approximately here
	\centering
    \includegraphics[scale = 0.33]{cbcv2.png}
    \vspace{-1cm} % Replace with your image filename
    \caption{Methodological Framework CBC-ML}
    \label{fig:CBC} % Optional: use \label for referencing
\end{figure*}

In CBC, after the generation and visualization of MF's and MP's, the unique bits that were produced by MFA are counted based on its presence and absence in the data set. Essentially, a series of structural queries were performed to determine the total frequency of the specific bit in the given data set. After determining the total frequency of each bits, they are ranked in descending order. From the ranking, top 10 bits were used to create a CBC-ML Model, which was expected to have a capacity to perceive and classify molecules bioactivity against HCT-116 (\autoref{fig:CBC}). 

\begin{figure*}[h] % 'h' places the figure approximately here
    \centering
    \hspace{-1.05cm}
    \includegraphics[scale=0.22]{csbcv2.png} % Replace with your image filename
    \caption{Methodological Framework CBC-ML}
    \label{fig:CSBC} % Optional: use \label for referencing
\end{figure*}

On the other hand, CSBC, applies blind clustering to the compound's bioactivity via K-Clustering. This is done through the calculation of euclidean distance (\ref{eq:euclid}) between the centroids and data points. Initially, k-seeding is perform --- wherein centroids are randomly place in the given data set, then its position is optimized base on its eucledian distance relative to a given data point (i.e., \% Inhibition), this continues until such time it reaches the minimum radius. In this study, the random state of the centroids in K-Clustering was set to 29. \footnote{if the random state is not set every run could have minimal variations.}    
 \begin{equation}
    d =\sqrt{(x_{2}-x_{1})^2 +(y_{2}-y_{1})^2}
\label{eq:euclid}
\end{equation}
where d is Euclidean distance, and $(x_{2}-x_{1}),(y_{2}-y_{1})$ are the coordinates of Molecule ID and \% Inhibition respectively. 

Before proceeding with K-Clustering, Within-Cluster Sum of Squares (WCSS) (\ref{eq:WCSS}) were optimized through the elbow method. Its goal is to find the number of clusters where WCSS values converge (i.e., it does not significantly change even after increasing the number of clusters). After parameter optimization of K-Clustering it was then used to cluster clean data set of HCT-116. 

\begin{equation}
   WCSS =\sum_{i=1}^{k}\sum_{j=1}^{n_{i}}distance(x^{i}_{j},c_{i})^{2}
   \label{eq:WCSS}
\end{equation}    
where $distance(x^{i}_{j},c_{i})$ represents the distance between j-th data points $x^{i}_{j}$ in centroid $c_{i}$ and the cluster i. 

After clustering, the 2D-structure matrix representation of each data point are labeled under the following categories: a) High Inhibition (HI); b) Moderate Inhibition (MI); c) Low Inhibition (LI); d) Very Low Inhibition (VLI); and e) No Inhibition (NI). Since each compounds has its own MP's represented by a 1-D matrix, they can be added, subtracted, or multiplied. To easily categorize which among the bits were more significant than the others, 1-D matrices of compounds under VLI were subtracted to VHI, MI, LI and NI categories (\autoref{fig:subtraction}). By doing so, the bits that appear to positively contribute to the bioactivity, or positive bits (PB), the bits that negatively contribute to the bioactivity, or negative bits (NB) and the bits that appear to have no influence on bioactivity, or non-significant bits (NSB) were identified. \footnote{PB and NB are both classified as significant bits for they positively or negatively affects the \%Inhibition of compounds against HCT-116.}. After Cluster Subtraction, the resulting bit difference underwent simplification (i.e., combination of same MP's); followed by bit frequency counting, ranking, and comparison (\autoref{fig:CSBC}). The top 16 most common bits were used for the creation of CSBC-Machine Learning Model (CSBC-ML).

\begin{figure}[h] % 'h' places the figure approximately here
	\centering
	\includegraphics[scale=0.18]{mol15_minus_mol135.png} % Replace with your image filename
	\vspace{-0.5cm}
	\begin{equation*}
		\begin{bmatrix}
			1 & 0 & 1 & 0 & 1 & 1 & 1 & \ldots & 1 \\ 
			0 & 1 & 1 & 0 & 1 & 1 & 1 & \ldots & 1 \\
			\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ldots & \vdots \\
			0 & 1 & 1 & 0 & 1 & 1 & 1 & \ldots & 1
		\end{bmatrix}
		\text{-}
		\begin{bmatrix}
			0 & 1 & 0 & 1 & 1 & 1 & 1 & \ldots & 1 \\ 
			0 & 1 & 0 & 1 & 1 & 1 & 1 & \ldots & 1 \\
			\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ldots & \vdots \\
			0 & 0 & 1 & 0 & 1 & 1 & 1 & \ldots & 1
		\end{bmatrix}
		\text{=}
		\begin{bmatrix}
			1 & 0 & 1 & -1 & 0 & 0 & 0 & \ldots & 1 \\ 
			0 & 0 & 1 & -1& 0 & 0 & 0 & \ldots & 1 \\
			\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ldots & \vdots \\
			0 & 0 & 1 & 0 & 0 & 0 & 0 & \ldots & 1
		\end{bmatrix}
	\end{equation*}
	\caption{Structure Subtraction Illustration}
	\vspace{-0.3cm}
	\label{fig:subtraction} % Optional: use \label for referencing
\end{figure}
      
\subsection*{Quantitative Structural Activity Relationship (QSAR): CBC-ML and CSBC-ML}
The top 10, and 16 most common structures from CBC and CSBC methods were used in the development of two ML models: CBC-ML and CSBC-ML. These are expected to accurately perceive and classify the compounds bioactivity against HCT-116 (active or inactive) based on their 2D-Structures. The following mathematical models were used in this study to develop the CBC-ML and CSBC-ML: a) Logistic Regression Model (Logit); b) XGBoost Model (XGB); c) Random Forest Model (RF); and d) Support Vector Machine Model (SVM). 

\subsubsection*{a. CBC-ML and CSBC-ML: Mathematical Models}
\FloatBarrier % Ensures table stays below subsection

\begin{table}[h]
	\centering
	\renewcommand{\arraystretch}{1.3}
	\small
	\begin{threeparttable} % Enables footnotes
		\begin{tabular}{>{\centering\arraybackslash}p{2cm} >{\centering\arraybackslash}p{6cm} >{\centering\arraybackslash}p{6cm}}
			\hline
			\textbf{Mathematical Models} \cite{SML-formulas} & \textbf{Equations} & \textbf{Cases} \\
			\hline
			Logit \tnote{a} &  
			\begin{equation}
				y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_nX_n + \epsilon
				\label{eq:logit}
			\end{equation} &  
			Performs well with linearly correlated data. \\  
			
			XGB \tnote{b} &  
			\begin{equation}
				\mathcal{L} = \sum_i l(y_i, \hat{y}_i) + \sum_k \Omega(f_k)
				\label{eq:xgboost}
			\end{equation} &  
			Can handle large datasets, imbalanced classes, and complex relationships. \\  
			
			RF \tnote{c} &  
			\begin{equation}
				f(x) = \frac{1}{M} \sum_{m=1}^M h_m(x)
				\label{eq:random}
			\end{equation} &  
			Works well with non-linear data, robust against noise and overfitting. \\  
			
			SVM \tnote{d} &  
			\begin{equation}  
				\min \frac{1}{2} w^2 \quad \text{subject to } y_i(w \cdot x_i + b) \geq 1
				\label{eq:svm}
			\end{equation} &  
			Suitable for high-dimensional spaces, performs well for non-linear separable problems, and resilient to overfitting. \\  
			\hline
		\end{tabular}
		\begin{tablenotes}
			\scriptsize
			\item[a] $ y $ is the target variable (\% Inhibition), $\beta_0$ is the intercept, $\beta_n$ are coefficients, and $\epsilon$ is the error term.
			\item[b] $ l(y_i, \hat{y}_i) $ is the loss function, and $ \Omega(f_k) $ is the regularization term controlling model complexity.
			\item[c] $ M $ is the total number of trees, and $ h_m(x) $ is the output from tree $ m $.
			\item[d] $ w $ is the weight vector defining the hyperplane, $ x_{i} $ are input features, $ y_{i} $ are target labels (\% Inhibition against HCT-116), and $ b $ is the bias term.
		\end{tablenotes}
	\end{threeparttable}
	\caption{Comparison of Mathematical Models, Equations, and Suitable Cases}
	\label{tab:math_models}
\end{table}

\FloatBarrier % Prevents table from floating too far

Logit models focuses (\ref{eq:logit}) on identifying the presence and absence of a specific feature in a given query. It is based on minimizing the residual sum of squares to find the optimal parameteric weights for the features. Fundamentally, CBC-ML and CSBC-ML were designed to accurately locate the target features (bits) in a given query (2-D structure of compounds), and to determine its specific weights. However, logit only works for linearly behaving data set, for non-linear behavior XGB, RF and SVM are best to implement due to the following reasons: a) XGB (\ref{eq:xgboost}) involves gradient boosting iteration that gives its advantage on improving its predictions by correcting residuals during the learning process; b) RF models builds an ensemble of decision trees that are averaged to result to a prediction; and c) SVM model finds the optimal hyperplane between the target features and variable by adjusting its kernel functions $(K(x,x'))$(\ref{eq:svm}) (\autoref{tab:math_models}). The data set used in the model development were partitioned in 80-20 manner and a random state of 29 was used.

%\subsubsection*{a. Logit Model of CBC-ML and CSBC-ML}
%Logit Models focuses (\ref{eq:logit}) on identifying the presence and absence of a specific feature in a given query. The model is based on minimizing the residual sum of squares to find the optimal parameteric weights for the features. Fundamentally, CBC-ML and CSBC-ML were designed to accurately locate the target features (bits) in a given query (2-D structure of compounds), and to determine its specific weights. The two top bits resulting from the CBC or CSBC method were used as the features for  the CBC-ML and CSBC-ML respectively. To train and test the models, clean data set was split into train and test groups via an 80-20 partition. The models produced from this procedure are named accordingly as CBC-ML-LOG and CSBC-ML-LOG. 
%To achieved the consistency in every runs, the random state is set to 29. 

%In the training sets, the models were trained to perceived and calculate the weights of each target features, and used them to predict the compounds potency against HCT-116. Whereas, test data set were used to validate the prediction produced by the model.     

%where y is the target variable (\% Inhibition), $\beta_0$ is the intercept, ($\beta_1$, $\beta_2$ , $\ldots$, and $\beta_n$) coefficients of each features (MF/bits),($X_1$, $X_2$, $\ldots$, $X_n$) are input features (i.e., 1 & 0) and $\epsilon$ is the random error term. 

%\subsubsection*{b. XGBoost Model of CBC-ML and CSBC-ML}
%In case that the features were not linearly correlated to the target variable (\%Inhibition against HCT-116), XGBoost model were employed (\ref{eq:xgboost}). This model involves gradient boosting iteration that gives its advantage on improving its predictions by correcting residuals during the learning process. Features used in this model were the same as the Logit model and the random state was set to 29. For the development of CBC-ML-XGB and CSBC-XGB, clean data set were partitioned into 80-20 (training, testing), and the features used are the top bits from CBC and CSBC respectively.    


%where ($l(y_i, \hat{y}_i)$) is the Loss function (e.g., Mean Squared Error or Log Loss) and ($\Omega(f_k)$) is Regularization term to control model complexity. 


%\subsubsection*{c. Random Forest Model of CBC-ML and CSBC-ML}
%Random Forest Model are also effective on handling non-linear data and known to be robust against noise and overfitting (\ref{eq:random}). It builds an ensemble of decision tree that are averaged to produced a prediction. Since, the top bits could have multicollinearity characteristics, it could be detected through Random Forest Model. However, compared to Logit and XGBoost models, it is computationally intensive. For the development of CBC-ML-RF and CSBC-ML-RF, clean data set were partitioned by 80-20 with a random state of 29, and the features used were the top bits from CBC and CSBC. 


%where (M) is the total number of trees, and ($h_m(x)$) is output from tree (m). 


%\subsubsection*{d. Support Vector Machine Model of CBC-ML and CSBC-ML}
%The features used in this study were the top bits which probably have high-dimensionality, if this is the case, SVM model fits the best. SVM model aims to find the optimal hyperplane between the target features and variable. In instances of non-linearity behavior, the model kernel functions ($K(x, x')$) can be adjusted to transform the feature space (\ref{eq:svm}), making it possible for the model to handle complex and high-dimensionality data sets. However, compared to other models presented, it is memory intensive and hard to interpret. The model produced from this mathematical model were called CBC-ML-SVM and CSBC-ML-SVM, wherein the partitioned clean data set and top bits were used as the target labels and input features respectively. 


%where (w) is weight vector defining the hyperplane, ($x_{i}$) are input features, ($y_{i}$) target labels (\%Inhibition against HCT-116) and (b) is the bias term. 

\subsubsection*{b. Comparative Analysis of QSAR-ML Models}
Results from the four different models were assessed based on the following: a); Accuracy and Precision through confusion matrix; b) Model fitting performance during training and testing; and c) overall ability of the model to distinguish between classes. The Confusion matrix is performance evaluation tool that can assess how well a model has predicted outcomes based on comparing the actual positive and negative to predicted values. It entails the information about accuracy(\ref{eq:accu}), precision(\ref{eq:prec}), recall (sensitivity)(\ref{eq:recall}), F1 score(\ref{eq:f1}), and specificity(\ref{eq:speci}). For each of the confusion matrix parameters true and false positive-negatives values were accounted.    

\begin{equation}
    \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
    \label{eq:accu}
\end{equation}

\begin{equation}
    \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
    \label{eq:prec}
\end{equation}

\begin{equation}
   \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
   \label{eq:recall}
\end{equation}

\begin{equation}
     \text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
     \label{eq:f1}
\end{equation}

\begin{equation}
    \text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}}
    \label{eq:speci}
\end{equation}

where (TP, TN) are true positive and negatives, (FP, FN) are false positive and negative values.

To check the fitting performance of each model, the confusion matrix parameters were compared during training and testing stage. To evaluate the overall capability of the model to distinguish between classes (active and inactive against HCT-116), ROC (\ref{eq:tpr},\ref{eq:fpr}), and AUC (\ref{eq:auc}) were used. AUC helps to summarize the results from ROC curve using the following criterion: a) AUC = 1.0 perfect classification; and b) AUC = 0.5 random guessing (no discriminatory power). A good model are expected to have the following: a) high accuracy and precision; b) a good fit (not over or under-fitted); c) AUC value of greater than 0.5. 

\begin{equation}
    \text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}
    \label{eq:tpr}
\end{equation}

\begin{equation}
    \text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}
    \label{eq:fpr}
\end{equation}

\begin{equation}
    \text{AUC} = \int_{0}^{1} \text{TPR}(\text{FPR}) , d(\text{FPR})
    \label{eq:auc}
\end{equation}
where TPR and FPR are the true and false positive rates.
%\subsubsection{}


